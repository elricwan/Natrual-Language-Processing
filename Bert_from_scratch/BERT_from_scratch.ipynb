{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "from transformers.file_utils import (\n",
    "    ModelOutput,\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    NextSentencePredictorOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import (\n",
    "    PreTrainedModel,\n",
    "    apply_chunking_to_forward,\n",
    "    find_pruneable_heads_and_indices,\n",
    "    prune_linear_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CONFIG_FOR_DOC = \"BertConfig\"\n",
    "_TOKENIZER_FOR_DOC = \"BertTokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"bert-large-uncased\",\n",
    "    \"bert-base-cased\",\n",
    "    \"bert-large-cased\",\n",
    "    \"bert-base-multilingual-uncased\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    \"bert-base-chinese\",\n",
    "    \"bert-base-german-cased\",\n",
    "    \"bert-large-uncased-whole-word-masking\",\n",
    "    \"bert-large-cased-whole-word-masking\",\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "    \"bert-large-cased-whole-word-masking-finetuned-squad\",\n",
    "    \"bert-base-cased-finetuned-mrpc\",\n",
    "    \"bert-base-german-dbmdz-cased\",\n",
    "    \"bert-base-german-dbmdz-uncased\",\n",
    "    \"cl-tohoku/bert-base-japanese\",\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "    \"cl-tohoku/bert-base-japanese-char\",\n",
    "    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\",\n",
    "    \"TurkuNLP/bert-base-finnish-cased-v1\",\n",
    "    \"TurkuNLP/bert-base-finnish-uncased-v1\",\n",
    "    \"wietsedv/bert-base-dutch-cased\",\n",
    "    # See all BERT models at https://huggingface.co/models?filter=bert\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n",
    "    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n",
    "    try:\n",
    "        import re\n",
    "\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "    except ImportError:\n",
    "        logger.error(\n",
    "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
    "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
    "        )\n",
    "        raise\n",
    "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
    "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
    "    # Load weights from TF model\n",
    "    init_vars = tf.train.list_variables(tf_path)\n",
    "    names = []\n",
    "    arrays = []\n",
    "    for name, shape in init_vars:\n",
    "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
    "        array = tf.train.load_variable(tf_path, name)\n",
    "        names.append(name)\n",
    "        arrays.append(array)\n",
    "\n",
    "    for name, array in zip(names, arrays):\n",
    "        name = name.split(\"/\")\n",
    "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
    "        # which are not required for using pretrained model\n",
    "        if any(\n",
    "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
    "            for n in name\n",
    "        ):\n",
    "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "            continue\n",
    "        pointer = model\n",
    "        for m_name in name:\n",
    "            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
    "                scope_names = re.split(r\"_(\\d+)\", m_name)\n",
    "            else:\n",
    "                scope_names = [m_name]\n",
    "            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n",
    "                pointer = getattr(pointer, \"bias\")\n",
    "            elif scope_names[0] == \"output_weights\":\n",
    "                pointer = getattr(pointer, \"weight\")\n",
    "            elif scope_names[0] == \"squad\":\n",
    "                pointer = getattr(pointer, \"classifier\")\n",
    "            else:\n",
    "                try:\n",
    "                    pointer = getattr(pointer, scope_names[0])\n",
    "                except AttributeError:\n",
    "                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
    "                    continue\n",
    "            if len(scope_names) >= 2:\n",
    "                num = int(scope_names[1])\n",
    "                pointer = pointer[num]\n",
    "        if m_name[-11:] == \"_embeddings\":\n",
    "            pointer = getattr(pointer, \"weight\")\n",
    "        elif m_name == \"kernel\":\n",
    "            array = np.transpose(array)\n",
    "        try:\n",
    "            assert (\n",
    "                pointer.shape == array.shape\n",
    "            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n",
    "        except AssertionError as e:\n",
    "            e.args += (pointer.shape, array.shape)\n",
    "            raise\n",
    "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
    "        pointer.data = torch.from_numpy(array)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1331,  0.2085,  0.1718],\n",
      "         [ 0.4032, -0.0933,  0.8349],\n",
      "         [-0.4728, -0.0840, -0.2545],\n",
      "         [ 1.0279,  1.2988, -0.1398]],\n",
      "\n",
      "        [[-0.4728, -0.0840, -0.2545],\n",
      "         [ 0.8380,  0.8190, -0.4794],\n",
      "         [ 0.4032, -0.0933,  0.8349],\n",
      "         [-0.6080,  2.0447, -0.6498]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.4996,  0.2196, -1.8582],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0671,  0.5463, -0.2623]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Show the function of embedding\n",
    "'''\n",
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding = nn.Embedding(10, 3)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "test = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "print(embedding(test))\n",
    "\n",
    "# example with padding_idx\n",
    "# With padding_idx set, the embedding vector at padding_idx is initialized to all zeros.\n",
    "# The gradient for this vector from Embedding is always zero.\n",
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "test = torch.LongTensor([[0,2,0,5]])\n",
    "embedding(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.2.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the parameters\n",
    "BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
    "            self.crossattention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            assert hasattr(\n",
    "                self, \"crossattention\"\n",
    "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "            if getattr(self.config, \"gradient_checkpointing\", False):\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOnlyNSPHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return seq_relationship_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = BertConfig\n",
    "    load_tf_weights = load_tf_weights_in_bert\n",
    "    base_model_prefix = \"bert\"\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPreTrainingOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Output type of :class:`~transformers.BertForPreTraining`.\n",
    "    Args:\n",
    "        loss (`optional`, returned when ``labels`` is provided, ``torch.FloatTensor`` of shape :obj:`(1,)`):\n",
    "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
    "            (classification) loss.\n",
    "        prediction_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
    "        seq_relationship_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n",
    "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
    "            before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
    "            sequence_length, sequence_length)`.\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    prediction_logits: torch.FloatTensor = None\n",
    "    seq_relationship_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "BERT_START_DOCSTRING = r\"\"\"\n",
    "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
    "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
    "    pruning heads etc.)\n",
    "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
    "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
    "    general usage and behavior.\n",
    "    Parameters:\n",
    "        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
    "            weights.\n",
    "\"\"\"\n",
    "\n",
    "BERT_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n",
    "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
    "            details.\n",
    "            `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
    "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
    "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
    "            1]``:\n",
    "            - 0 corresponds to a `sentence A` token,\n",
    "            - 1 corresponds to a `sentence B` token.\n",
    "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
    "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
    "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
    "            config.max_position_embeddings - 1]``.\n",
    "            `What are position IDs? <../glossary.html#position-ids>`_\n",
    "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
    "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
    "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
    "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
    "            vectors than the model's internal embedding lookup matrix.\n",
    "        output_attentions (:obj:`bool`, `optional`):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
    "            tensors for more detail.\n",
    "        output_hidden_states (:obj:`bool`, `optional`):\n",
    "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
    "            more detail.\n",
    "        return_dict (:obj:`bool`, `optional`):\n",
    "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
    "    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
    "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
    "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
    "    input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    @add_code_sample_docstrings(\n",
    "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=\"bert-base-uncased\",\n",
    "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            batch_size, seq_length = input_shape\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size, seq_length = input_shape\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5296810082ae41e4abe9ca002a08259e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=433, style=ProgressStyle(description_width="
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdeef1997a994a4b83713f9baa77f868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=435779157, style=ProgressStyle(description_"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's load a BERT model for TensorFlow and PyTorch\n",
    "model_pt = BertModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers generates a ready to use dictionary with all the required parameters for the specific framework.\n",
    "input_pt = tokenizer(\"This is a sample input\", return_tensors=\"pt\")\n",
    "\n",
    "# Let's compare the outputs\n",
    "output_pt = model_pt(**input_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2940,  0.0614,  0.2065,  ..., -0.3295,  0.3305, -0.0998],\n",
       "         [ 0.2962, -0.5739,  0.5813,  ..., -0.3564,  0.0185, -0.0938],\n",
       "         [ 0.2031, -0.1914,  0.1796,  ..., -0.0217,  0.1019,  0.0472],\n",
       "         ...,\n",
       "         [-0.2330, -0.3442,  0.7847,  ...,  0.0938,  0.1067,  0.1132],\n",
       "         [ 0.1223, -0.1405,  0.4740,  ..., -0.2991,  0.4244,  0.1603],\n",
       "         [ 0.0275, -0.4122,  0.4287,  ...,  0.2497,  0.8857, -0.6121]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-7.4945e-01,  4.5809e-01,  9.9991e-01, -9.9513e-01,  9.5136e-01,\n",
       "          8.8675e-01,  9.8779e-01, -9.7174e-01, -9.8115e-01, -4.0965e-01,\n",
       "          9.8255e-01,  9.9908e-01, -9.9485e-01, -9.9980e-01,  6.2276e-01,\n",
       "         -9.7763e-01,  9.8719e-01, -5.0504e-01, -9.9997e-01, -4.7105e-01,\n",
       "         -3.7424e-01, -9.9989e-01,  3.2504e-01,  9.4333e-01,  9.7932e-01,\n",
       "          9.3428e-02,  9.9002e-01,  9.9997e-01,  8.4371e-01,  1.6332e-02,\n",
       "          2.6779e-01, -9.9370e-01,  8.2270e-01, -9.9915e-01,  2.5660e-01,\n",
       "          2.4148e-01,  1.3677e-01, -2.1879e-01,  6.4477e-01, -9.3991e-01,\n",
       "         -6.4375e-01, -3.4332e-01,  4.7961e-01, -4.7986e-01,  7.9701e-01,\n",
       "          3.0270e-01,  2.3536e-01,  5.3573e-02, -1.5115e-01,  9.9986e-01,\n",
       "         -9.7186e-01,  9.9967e-01, -9.7687e-01,  9.9827e-01,  9.9732e-01,\n",
       "          4.1913e-01,  9.9650e-01,  1.1477e-01, -9.9473e-01,  3.0533e-01,\n",
       "          9.5507e-01,  6.0354e-02,  8.6282e-01, -2.8739e-01, -9.6662e-02,\n",
       "         -5.4097e-01, -7.6300e-01,  1.9284e-01, -5.6279e-01,  2.3507e-01,\n",
       "          3.9851e-01,  2.2931e-01,  9.8770e-01, -9.0177e-01, -1.0087e-01,\n",
       "         -9.3309e-01,  6.0612e-01, -9.9992e-01,  9.2953e-01,  9.9996e-01,\n",
       "          4.7713e-01, -9.9973e-01,  9.9664e-01, -3.4924e-01, -5.5235e-01,\n",
       "          2.8796e-01, -9.9076e-01, -9.9963e-01,  1.0018e-01, -5.2720e-01,\n",
       "          8.2861e-01, -9.9024e-01,  2.8478e-01, -8.3477e-01,  9.9998e-01,\n",
       "         -8.5181e-01, -1.8735e-01,  4.6076e-01,  8.9936e-01, -4.8081e-01,\n",
       "         -5.8376e-01,  8.8528e-01,  9.8749e-01, -9.5097e-01,  9.9374e-01,\n",
       "          4.0213e-01, -9.2359e-01, -8.4576e-01,  5.1531e-01,  1.0369e-01,\n",
       "          9.8461e-01, -9.8776e-01, -7.6208e-01,  9.1847e-02,  9.5451e-01,\n",
       "         -7.3498e-01,  9.9242e-01,  7.4705e-01, -3.3181e-01,  9.9998e-01,\n",
       "         -1.3582e-01,  9.0162e-01,  9.9898e-01,  6.0277e-01, -5.1195e-01,\n",
       "         -1.2744e-01, -2.6908e-01,  8.6047e-01, -2.6056e-01, -2.7638e-01,\n",
       "          7.4729e-01, -9.9474e-01, -9.8918e-01,  9.9969e-01, -2.3768e-01,\n",
       "          9.9998e-01, -9.9938e-01,  9.8335e-01, -9.9995e-01, -7.7361e-01,\n",
       "         -5.9787e-01,  3.9627e-03, -9.4182e-01,  3.8206e-01,  9.9525e-01,\n",
       "          9.5476e-02, -8.9245e-01, -7.6094e-01,  5.6117e-01, -7.6146e-01,\n",
       "          4.4286e-01,  7.3153e-01, -9.5084e-01,  9.9876e-01,  9.8729e-01,\n",
       "          9.1511e-01,  9.7516e-01,  1.9391e-01, -8.5151e-01,  7.6689e-01,\n",
       "          9.8813e-01, -9.9954e-01,  7.9522e-01, -9.8042e-01,  9.9940e-01,\n",
       "          9.7451e-01,  6.1768e-01, -9.7586e-01,  9.9992e-01, -4.7084e-01,\n",
       "          2.6257e-01, -2.4329e-01, -9.0598e-02, -9.9065e-01,  4.9063e-01,\n",
       "          3.8195e-01,  7.1537e-01,  9.9972e-01, -9.9193e-01,  9.9952e-01,\n",
       "          9.8799e-01,  1.8103e-01,  7.6872e-01,  9.9295e-01, -9.9733e-01,\n",
       "         -9.8032e-01, -9.8985e-01,  4.3940e-01,  3.2902e-01,  7.1680e-01,\n",
       "          2.2705e-01,  9.5720e-01,  9.8733e-01,  5.3905e-01, -9.9773e-01,\n",
       "         -2.8211e-01,  9.8251e-01, -3.4624e-01,  9.9997e-01, -1.4105e-01,\n",
       "         -9.9984e-01, -7.7552e-01,  8.4241e-01,  9.9297e-01, -2.8917e-01,\n",
       "          9.8283e-01, -5.1647e-01, -7.0637e-02,  9.5897e-01, -9.9904e-01,\n",
       "          9.7154e-01,  2.7019e-01,  7.6470e-01,  8.7084e-01,  9.9564e-01,\n",
       "         -4.6812e-01, -2.3058e-01,  3.2215e-01, -5.9371e-01,  9.9991e-01,\n",
       "         -9.9971e-01, -3.7322e-01,  5.4242e-01, -9.9606e-01, -9.9872e-01,\n",
       "          9.8418e-01, -5.6293e-02, -7.7153e-01, -2.4123e-01,  1.0368e-01,\n",
       "          3.3044e-01,  8.1798e-01,  9.9251e-01, -3.3840e-01, -2.1729e-01,\n",
       "         -9.9988e-01, -9.6237e-01, -7.0319e-01, -9.2864e-01,  5.4727e-03,\n",
       "          6.4167e-01, -3.5794e-01, -8.9784e-01, -9.8425e-01,  9.6243e-01,\n",
       "          7.6279e-01, -8.1262e-01, -1.0032e-01, -1.6746e-01, -9.8617e-01,\n",
       "          2.3260e-01, -7.5741e-01, -9.9907e-01,  9.9970e-01, -6.1073e-01,\n",
       "          9.8069e-01,  9.7814e-01, -9.9594e-01,  4.8362e-01, -9.8732e-01,\n",
       "          2.7184e-02, -9.9903e-01,  3.6992e-01,  2.9152e-01, -5.7437e-01,\n",
       "          5.3113e-02,  9.9539e-01, -9.6686e-01, -7.2832e-01,  5.9457e-01,\n",
       "         -9.9989e-01,  9.3682e-01, -1.3113e-01,  9.9931e-01,  7.9801e-02,\n",
       "          5.6826e-01,  9.9012e-01,  9.0308e-01, -9.8873e-01, -9.9985e-01,\n",
       "          8.0980e-01,  9.9472e-01, -9.9658e-01, -2.5675e-01,  9.9995e-01,\n",
       "         -9.7100e-01, -6.5359e-01, -9.4843e-01, -9.9742e-01, -9.9971e-01,\n",
       "          1.2681e-01, -4.2719e-01,  1.6879e-01,  9.8710e-01,  3.0150e-01,\n",
       "          1.8538e-01,  9.9625e-01,  9.9734e-01,  1.5138e-01, -1.5754e-01,\n",
       "          7.2417e-02, -9.7274e-01, -9.9492e-01,  5.7499e-01,  1.9127e-01,\n",
       "         -9.9997e-01,  9.9988e-01, -9.9594e-01,  9.9933e-01,  8.1617e-01,\n",
       "         -9.6679e-01,  7.7559e-01,  2.3952e-02, -8.8754e-01,  3.0951e-02,\n",
       "          9.9992e-01,  9.8234e-01, -1.0622e-01,  2.4346e-01,  8.2693e-01,\n",
       "         -1.2230e-01,  3.8287e-01, -5.7787e-01, -4.1501e-01,  2.3289e-01,\n",
       "         -9.4062e-01,  9.7702e-01,  5.8477e-01, -9.9378e-01,  9.7899e-01,\n",
       "          1.2538e-01,  6.3740e-01, -4.7486e-01,  8.4574e-01,  9.9337e-01,\n",
       "         -1.3473e-01, -3.5786e-01, -7.3033e-02, -9.3289e-01, -9.5140e-01,\n",
       "          1.8709e-01, -9.9098e-01, -2.3370e-01,  8.9689e-01,  9.8557e-01,\n",
       "         -9.9013e-01,  9.9780e-01, -1.7669e-01,  8.5164e-01, -9.8364e-01,\n",
       "          9.9998e-01, -9.9822e-01,  9.7601e-02,  5.3023e-01, -4.9592e-01,\n",
       "          1.8909e-02,  9.9406e-01,  9.3895e-01,  9.5476e-01, -9.1014e-01,\n",
       "         -4.6818e-01,  8.7242e-01,  9.6736e-01, -9.2276e-01,  4.6970e-02,\n",
       "         -9.9042e-01, -4.3581e-01,  9.9523e-01,  9.8353e-01, -6.8951e-02,\n",
       "         -7.0287e-01, -9.8920e-01,  9.5561e-01, -6.6751e-01, -8.8010e-01,\n",
       "         -3.4655e-02, -7.5024e-01,  5.3563e-01,  9.8686e-01, -2.0295e-01,\n",
       "          6.4872e-01,  1.0203e-01, -9.9342e-01,  7.1036e-01,  7.5228e-01,\n",
       "          9.9986e-01, -9.8204e-01,  5.2039e-01,  9.9152e-01, -2.1384e-01,\n",
       "         -6.0941e-01,  4.4241e-01,  9.9434e-01, -9.6602e-01, -2.4898e-01,\n",
       "         -9.9963e-01, -8.8012e-02, -7.6569e-01,  4.3049e-02, -4.4504e-01,\n",
       "          1.7595e-01, -7.1118e-01,  9.0859e-01,  2.6408e-01,  7.3265e-01,\n",
       "         -4.0004e-01,  9.7194e-01, -3.8256e-01, -1.0762e-01, -4.0651e-01,\n",
       "         -1.1063e-01,  4.7195e-01,  4.9258e-02,  9.8472e-01, -9.6573e-01,\n",
       "          9.9989e-01, -7.3410e-02, -9.9997e-01, -9.7233e-01, -6.7082e-01,\n",
       "         -9.9976e-01,  2.3987e-01, -9.9763e-01,  9.9111e-01,  8.9100e-01,\n",
       "         -9.8165e-01, -9.8967e-01, -9.9863e-01, -9.9860e-01,  6.1083e-01,\n",
       "          5.1471e-01, -4.8746e-04,  1.1756e-01,  8.8396e-01,  1.3479e-01,\n",
       "         -3.5645e-01, -2.0521e-01, -9.4460e-01, -7.0752e-01, -9.7852e-01,\n",
       "          5.0112e-01, -9.9997e-01, -7.0379e-01,  9.9211e-01, -9.8838e-01,\n",
       "         -7.7674e-01, -9.3837e-01, -6.1413e-01, -8.4422e-01,  5.9788e-01,\n",
       "          9.8723e-01, -9.8159e-02, -5.0875e-01, -9.9969e-01,  9.9190e-01,\n",
       "         -4.7918e-01,  1.8162e-01, -7.8556e-01, -9.8030e-01,  9.9980e-01,\n",
       "          8.7203e-01, -4.3696e-02, -2.5354e-01, -9.9918e-01,  9.5546e-01,\n",
       "         -8.1378e-01, -9.0325e-01, -9.7945e-01,  7.4337e-02, -9.4444e-01,\n",
       "         -9.9991e-01,  5.7075e-02,  9.7044e-01,  9.9712e-01,  9.7804e-01,\n",
       "          2.5242e-01, -3.6533e-01, -9.6635e-01,  1.7496e-01, -9.9996e-01,\n",
       "          6.9040e-01,  7.2718e-01, -9.8682e-01, -5.8768e-01,  9.9303e-01,\n",
       "          9.8319e-01, -8.7797e-01, -9.6830e-01,  7.6434e-01,  5.0531e-01,\n",
       "          9.6351e-01, -2.3199e-01, -5.6186e-01,  2.1279e-01, -9.7492e-02,\n",
       "         -9.8764e-01, -9.1900e-01,  9.9767e-01, -9.8894e-01,  9.8163e-01,\n",
       "          9.7965e-01,  9.8787e-01, -3.9564e-02, -4.3509e-02, -9.5274e-01,\n",
       "         -9.9663e-01, -4.9970e-01,  2.0001e-01, -9.9994e-01,  9.9991e-01,\n",
       "         -9.9997e-01,  4.4549e-01, -4.8387e-01,  7.0930e-01,  9.8756e-01,\n",
       "         -3.3013e-01, -9.9993e-01, -9.9989e-01, -4.5236e-01, -1.5357e-01,\n",
       "          9.9390e-01,  2.6181e-01,  2.1000e-01, -3.7682e-01, -2.9383e-01,\n",
       "          9.9868e-01, -3.2388e-01, -3.1840e-01, -9.7161e-01,  9.9972e-01,\n",
       "          5.7337e-01, -9.9916e-01,  9.8379e-01, -9.9969e-01,  7.6886e-01,\n",
       "          9.7764e-01,  8.8926e-01,  9.6745e-01, -9.9100e-01,  9.9997e-01,\n",
       "         -9.9986e-01,  9.9794e-01, -9.9998e-01, -9.8834e-01,  9.9986e-01,\n",
       "         -9.9266e-01, -6.1124e-01, -9.9976e-01, -9.9160e-01,  5.8573e-01,\n",
       "          2.1428e-01, -5.5178e-01,  9.9288e-01, -9.9984e-01, -9.9879e-01,\n",
       "          2.3868e-01, -7.3078e-01, -5.8308e-01,  9.7928e-01, -4.2346e-01,\n",
       "          9.9526e-01,  2.7090e-02,  9.3969e-01,  2.0288e-01,  9.9202e-01,\n",
       "          9.9919e-01, -7.0391e-01, -9.0130e-01, -9.9506e-01,  9.8100e-01,\n",
       "         -4.5928e-01,  4.0595e-01,  9.5367e-01, -6.5586e-02, -7.5337e-01,\n",
       "          3.4421e-01, -9.9829e-01,  3.8911e-01, -5.0257e-01,  9.0140e-01,\n",
       "          7.9393e-01,  8.1339e-01,  2.8267e-04, -4.0899e-01, -1.0613e-01,\n",
       "         -9.9595e-01,  5.0178e-01, -9.9961e-01,  9.6554e-01, -8.3510e-01,\n",
       "          8.3474e-02, -3.2666e-01,  4.0082e-01, -9.4704e-01,  9.9966e-01,\n",
       "          9.9870e-01, -9.9954e-01,  9.3130e-03,  9.9184e-01, -5.6394e-01,\n",
       "          9.6991e-01, -9.9490e-01,  3.4391e-02,  9.4834e-01, -6.9073e-01,\n",
       "          9.8179e-01,  3.5468e-01, -9.4402e-02,  9.7681e-01, -9.9731e-01,\n",
       "         -7.8459e-01, -6.2093e-01,  1.3411e-01, -1.8802e-01, -9.8066e-01,\n",
       "          1.4819e-01,  9.5316e-01, -2.9952e-01, -9.9978e-01,  7.1618e-01,\n",
       "         -9.9962e-01, -1.5099e-01,  9.8704e-01,  3.6347e-01,  9.9992e-01,\n",
       "         -5.3954e-01, -1.2572e-02,  9.1176e-02, -9.9984e-01, -9.9332e-01,\n",
       "          7.5275e-02, -1.6861e-01, -8.7651e-01,  9.9029e-01, -3.1481e-02,\n",
       "          7.3793e-01, -9.9992e-01,  3.0009e-01,  9.8418e-01,  3.2517e-01,\n",
       "          7.3708e-01, -3.8666e-01, -9.3391e-01, -9.3091e-01, -6.0566e-01,\n",
       "          2.2041e-02,  7.9881e-01, -9.8889e-01, -4.2906e-01, -6.9470e-01,\n",
       "          9.9997e-01, -9.9868e-01, -8.6220e-01, -9.8806e-01,  4.1412e-01,\n",
       "          7.6189e-01,  4.0960e-01, -2.5290e-02, -7.3541e-01,  8.6304e-01,\n",
       "         -8.8725e-01,  9.9767e-01, -9.9662e-01, -9.9719e-01,  9.9984e-01,\n",
       "          3.5643e-01, -9.7633e-01, -4.3949e-01, -3.1768e-01,  1.8018e-01,\n",
       "          2.8061e-02,  6.9382e-01, -8.4972e-01, -1.4821e-01, -9.9875e-01,\n",
       "          5.4104e-01, -5.6863e-01, -9.9310e-01, -4.0265e-01, -4.3175e-01,\n",
       "         -9.9944e-01,  9.9495e-01,  9.7303e-01,  9.9995e-01, -9.9982e-01,\n",
       "          7.6847e-01,  6.9063e-02,  9.9936e-01, -1.8863e-02, -6.6359e-01,\n",
       "          7.9883e-01,  9.9976e-01, -4.7896e-01,  4.8458e-01,  9.6129e-02,\n",
       "         -6.7241e-02,  2.6881e-01, -6.6869e-01,  9.8633e-01, -8.4283e-01,\n",
       "          4.8699e-01, -9.8566e-01, -9.9993e-01,  9.9993e-01, -8.1330e-02,\n",
       "          9.9277e-01,  3.3510e-01,  7.2459e-01, -8.4037e-01,  9.5977e-01,\n",
       "         -9.5951e-01, -8.8130e-01, -9.9998e-01,  3.7522e-01, -9.9964e-01,\n",
       "         -9.9488e-01, -1.1763e-01,  9.9217e-01, -9.9966e-01, -9.9228e-01,\n",
       "         -2.6883e-01, -9.9998e-01,  8.0629e-01, -9.8027e-01, -5.0837e-01,\n",
       "         -9.9172e-01,  9.7679e-01, -3.3475e-01, -2.6246e-01,  9.7154e-01,\n",
       "         -9.7343e-01,  8.9388e-01,  7.5856e-01,  6.1538e-01,  4.1398e-01,\n",
       "          1.7052e-01, -6.2198e-01, -9.8410e-01, -8.2841e-01, -9.6271e-01,\n",
       "          5.8798e-01, -9.9121e-01, -7.3861e-01,  9.9812e-01,  9.8964e-01,\n",
       "         -9.9945e-01, -9.9762e-01,  9.8420e-01, -2.8163e-01,  9.9354e-01,\n",
       "         -4.1061e-01, -9.9988e-01, -9.9993e-01,  1.5134e-01, -2.9379e-01,\n",
       "          9.9729e-01, -4.0922e-01,  9.9793e-01,  6.3337e-01, -1.4489e-01,\n",
       "          3.2370e-01, -3.6542e-01, -2.7572e-01, -4.8170e-01, -1.1283e-01,\n",
       "          9.9997e-01, -6.9860e-01,  9.9298e-01]], grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer_Models",
   "language": "python",
   "name": "transformer_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
