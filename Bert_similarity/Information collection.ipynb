{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForQuestionAnswering, BertTokenizer,\n",
    "                                  XLMConfig, XLMForQuestionAnswering,\n",
    "                                  XLMTokenizer, XLNetConfig,\n",
    "                                  XLNetForQuestionAnswering,\n",
    "                                  XLNetTokenizer,\n",
    "                                  DistilBertConfig, DistilBertForQuestionAnswering, DistilBertTokenizer)\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from utils_squad import (read_squad_examples, convert_examples_to_features,\n",
    "                         RawResult, write_predictions,\n",
    "                         RawResultExtended, write_predictions_extended)\n",
    "from utils_squad_evaluate import EVAL_OPTS, main as evaluate_on_squad\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.visible_device_list = \"0\" #only the gpu 0 is allowed\n",
    "\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.01\n",
    "\n",
    "#set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) \\\n",
    "                  for conf in (BertConfig, XLNetConfig, XLMConfig)), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForQuestionAnswering, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForQuestionAnswering, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForQuestionAnswering, XLMTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForQuestionAnswering, DistilBertTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class struct():\n",
    "    def __init__(self):\n",
    "        self.train_file = 'train-v2.0.json'\n",
    "        self.predict_file = 'dev-v2.0.json'\n",
    "        # exam the new dataset\n",
    "        #self.predict_file = 'test.json'\n",
    "        self.model_type = 'bert'\n",
    "        self.model_name = 'bert-large-uncased-whole-word-masking'\n",
    "        self.task_name = 'MRPC'\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.do_lower_case = True\n",
    "        self.data_dir = 'GLUE_DIR/MRPC/'\n",
    "        self.max_seq_length = 128\n",
    "        self.per_gpu_eval_batch_size = 8\n",
    "        self.per_gpu_train_batch_size = 8\n",
    "        # the default is 2, we make it quicker\n",
    "        self.num_train_epochs = 3.0\n",
    "        self.learning_rate = 4e-5\n",
    "        self.output_dir = 'tmp/mrpc_output/'\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True   \n",
    "        self.local_rank = -1\n",
    "        self.version_2_with_negative = True\n",
    "        self.doc_stride = 128\n",
    "        self.max_query_length = 64\n",
    "        self.n_gpu = 1\n",
    "        self.max_steps = -1\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.weight_decay = 0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1\n",
    "        self.warmup_steps = 0\n",
    "        self.n_best_size = 20\n",
    "        self.max_answer_length = 30\n",
    "        self.verbose_logging = True\n",
    "        self.logging_steps = 50\n",
    "        self.save_steps = 5000\n",
    "        self.fp16 = True\n",
    "        self.fp16_opt_level = 'O1'\n",
    "        self.seed = 42\n",
    "        self.no_cuda = False\n",
    "        self.evaluate_during_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = struct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up device\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "args.model_type = args.model_type.lower()\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.model_name)\n",
    "model = model_class.from_pretrained(args.output_dir)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename1 = 'Thai_cook.json'\n",
    "with open(filename1, 'r') as f:\n",
    "    Thai = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick several data from the reviews\n",
    "data = {}\n",
    "idx = 1\n",
    "for i in range(1,len(Thai)+1):\n",
    "    key = str(i)\n",
    "    content = Thai[key]['review']\n",
    "    content = content.replace('(Translated by Google) ','')\n",
    "    content = content.split('(Original)')[0]\n",
    "    content = content.replace('\\n\\n','')\n",
    "    content.replace('\\n','')\n",
    "    Thai[key]['review'] = content\n",
    "    if len(content) > 0:\n",
    "        data[idx] = content\n",
    "        idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create similar input data\n",
    "test_grandson = {}\n",
    "test_grandson['qas'] = [{'question':'What to eat?',\n",
    "                         'id': '1',\n",
    "                          'answers': [{'text':'food','answer_start': 0}],\n",
    "                          'is_impossible': False},\n",
    "                        {'question':'What to try?',\n",
    "                         'id': '2',\n",
    "                          'answers': [{'text':'food','answer_start': 0}],\n",
    "                          'is_impossible': False},\n",
    "                       {'question':'What is good?',\n",
    "                         'id': '3',\n",
    "                          'answers': [{'text':'food','answer_start': 0}],\n",
    "                          'is_impossible': False},\n",
    "                       {'question':'What is delicious?',\n",
    "                         'id': '4',\n",
    "                          'answers': [{'text':'food','answer_start': 0}],\n",
    "                          'is_impossible': False},\n",
    "                       {'question':'What to recommend?',\n",
    "                         'id': '5',\n",
    "                          'answers': [{'text':'food','answer_start': 0}],\n",
    "                          'is_impossible': False},\n",
    "                       {'question':'What do you prefer?',\n",
    "                         'id': '6',\n",
    "                          'answers': [{'text':'food','answer_start': 0}],\n",
    "                          'is_impossible': False},\n",
    "                       {'question':'How is the service?',\n",
    "                         'id': '7',\n",
    "                          'answers': [{'text':'food','answer_start': 0}],\n",
    "                          'is_impossible': False},\n",
    "                        {'question':'How is the price?',\n",
    "                         'id': '8',\n",
    "                          'answers': [{'text':'food','answer_start': 0}],\n",
    "                          'is_impossible': False},\n",
    "                       {'question':'How long to wait in this place?',\n",
    "                         'id': '9',\n",
    "                          'answers': [{'text':'food','answer_start': 0}],\n",
    "                          'is_impossible': False},\n",
    "                       {'question':'Is the place clean?',\n",
    "                         'id': '10',\n",
    "                          'answers': [{'text':'food','answer_start': 0}],\n",
    "                          'is_impossible': True}]\n",
    "test_grandson['context'] = context\n",
    "\n",
    "test_child = {}\n",
    "test_child['title'] =  'Thai'\n",
    "test_child['paragraphs'] = [test_grandson]\n",
    "\n",
    "test = {}\n",
    "test['version'] = 'test'\n",
    "test['data'] = [test_child]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test to json\n",
    "with open('test.json', 'w') as outfile:\n",
    "    json.dump(test, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to change the datasets\n",
    "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Load data features from cache or dataset file\n",
    "    input_file = args.predict_file if evaluate else args.train_file\n",
    "    cached_features_file = 'cached_features_file'\n",
    "    test_features_file = 'test_features_file'\n",
    "    \n",
    "    logger.info(\"Creating features from dataset file at %s\", input_file)\n",
    "    examples = read_squad_examples(input_file=input_file,\n",
    "                                        is_training=not evaluate,\n",
    "                                        version_2_with_negative=args.version_2_with_negative)\n",
    "    features = convert_examples_to_features(examples=examples,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        max_seq_length=args.max_seq_length,\n",
    "                                        doc_stride=args.doc_stride,\n",
    "                                        max_query_length=args.max_query_length,\n",
    "                                            is_training=not evaluate)\n",
    "    \n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
    "    all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
    "    if evaluate:\n",
    "        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                                all_example_index, all_cls_index, all_p_mask)\n",
    "    else:\n",
    "        all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
    "        all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
    "        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                                all_start_positions, all_end_positions,\n",
    "                                all_cls_index, all_p_mask)\n",
    "\n",
    "    if output_examples:\n",
    "        return dataset, examples, features\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "\n",
    "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(dataset) if args.local_rank == -1 else DistributedSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    all_results = []\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': None if args.model_type == 'xlm' else batch[2]  # XLM don't use segment_ids\n",
    "                      }\n",
    "            example_indices = batch[3]\n",
    "            if args.model_type in ['xlnet', 'xlm']:\n",
    "                inputs.update({'cls_index': batch[4],\n",
    "                               'p_mask':    batch[5]})\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        for i, example_index in enumerate(example_indices):\n",
    "            eval_feature = features[example_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "            if args.model_type in ['xlnet', 'xlm']:\n",
    "                # XLNet uses a more complex post-processing procedure\n",
    "                result = RawResultExtended(unique_id            = unique_id,\n",
    "                                           start_top_log_probs  = to_list(outputs[0][i]),\n",
    "                                           start_top_index      = to_list(outputs[1][i]),\n",
    "                                           end_top_log_probs    = to_list(outputs[2][i]),\n",
    "                                           end_top_index        = to_list(outputs[3][i]),\n",
    "                                           cls_logits           = to_list(outputs[4][i]))\n",
    "            else:\n",
    "                result = RawResult(unique_id    = unique_id,\n",
    "                                   start_logits = to_list(outputs[0][i]),\n",
    "                                   end_logits   = to_list(outputs[1][i]))\n",
    "            all_results.append(result)\n",
    "\n",
    "    # Compute predictions\n",
    "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
    "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "    if args.version_2_with_negative:\n",
    "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(prefix))\n",
    "    else:\n",
    "        output_null_log_odds_file = None\n",
    "\n",
    "    if args.model_type in ['xlnet', 'xlm']:\n",
    "        # XLNet uses a more complex post-processing procedure\n",
    "        write_predictions_extended(examples, features, all_results, args.n_best_size,\n",
    "                        args.max_answer_length, output_prediction_file,\n",
    "                        output_nbest_file, output_null_log_odds_file, args.predict_file,\n",
    "                        model.config.start_n_top, model.config.end_n_top,\n",
    "                        args.version_2_with_negative, tokenizer, args.verbose_logging)\n",
    "    else:\n",
    "        write_predictions(examples, features, all_results, args.n_best_size,\n",
    "                        args.max_answer_length, args.do_lower_case, output_prediction_file,\n",
    "                        output_nbest_file, output_null_log_odds_file, args.verbose_logging,\n",
    "                        args.version_2_with_negative, args.null_score_diff_threshold)\n",
    "\n",
    "    # Evaluate with the official SQuAD script\n",
    "    evaluate_options = EVAL_OPTS(data_file=args.predict_file,\n",
    "                                 pred_file=output_prediction_file,\n",
    "                                 na_prob_file=output_null_log_odds_file)\n",
    "    results = evaluate_on_squad(evaluate_options)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the QA for the new dataset\n",
    "# evaluate the final results\n",
    "args.predict_file = 'test.json'\n",
    "dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "# Note that DistributedSampler samples randomly\n",
    "eval_sampler = SequentialSampler(dataset) if args.local_rank == -1 else DistributedSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 10.31it/s]\n"
     ]
    }
   ],
   "source": [
    "prefix=1\n",
    "args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "logger.info(\"  Num examples = %d\", len(dataset))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "all_results = []\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(args.device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'token_type_ids': None if args.model_type == 'xlm' else batch[2]  # XLM don't use segment_ids\n",
    "                    }\n",
    "        example_indices = batch[3]\n",
    "        if args.model_type in ['xlnet', 'xlm']:\n",
    "            inputs.update({'cls_index': batch[4],\n",
    "                            'p_mask':    batch[5]})\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    for i, example_index in enumerate(example_indices):\n",
    "        eval_feature = features[example_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "        if args.model_type in ['xlnet', 'xlm']:\n",
    "            # XLNet uses a more complex post-processing procedure\n",
    "            result = RawResultExtended(unique_id            = unique_id,\n",
    "                                        start_top_log_probs  = to_list(outputs[0][i]),\n",
    "                                        start_top_index      = to_list(outputs[1][i]),\n",
    "                                        end_top_log_probs    = to_list(outputs[2][i]),\n",
    "                                        end_top_index        = to_list(outputs[3][i]),\n",
    "                                        cls_logits           = to_list(outputs[4][i]))\n",
    "        else:\n",
    "            result = RawResult(unique_id    = unique_id,\n",
    "                                start_logits = to_list(outputs[0][i]),\n",
    "                                end_logits   = to_list(outputs[1][i]))\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
    "output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "# assume some question does not have an answer\n",
    "if args.version_2_with_negative:\n",
    "    output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.null_score_diff_threshold = 0\n",
    "args.version_2_with_negative = True\n",
    "if args.model_type in ['xlnet', 'xlm']:\n",
    "    # XLNet uses a more complex post-processing procedure\n",
    "    write_predictions_extended(examples, features, all_results, args.n_best_size,\n",
    "                    args.max_answer_length, output_prediction_file,\n",
    "                    output_nbest_file, output_null_log_odds_file, args.predict_file,\n",
    "                    model.config.start_n_top, model.config.end_n_top,\n",
    "                    args.version_2_with_negative, tokenizer, args.verbose_logging)\n",
    "else:\n",
    "    write_predictions(examples, features, all_results, args.n_best_size,\n",
    "                    args.max_answer_length, args.do_lower_case, output_prediction_file,\n",
    "                    output_nbest_file, output_null_log_odds_file, args.verbose_logging,\n",
    "                    args.version_2_with_negative, args.null_score_diff_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the answer\n",
    "import json\n",
    "filename1 = output_prediction_file\n",
    "with open(filename1, 'r') as f:\n",
    "    answer = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'Vegetarian dishes were bland and dry',\n",
       " '2': 'Pad Thai (both chicken and shrimp) and bowls were very good. Thai ice tea was delicious. Vegetarian dishes were bland and dry',\n",
       " '3': 'Pad Thai (both chicken and shrimp) and bowls were very good',\n",
       " '4': 'Thai ice tea',\n",
       " '5': '',\n",
       " '6': '',\n",
       " '7': '',\n",
       " '8': '',\n",
       " '9': '',\n",
       " '10': ''}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 10.59it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 10.60it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.52it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.52it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 10.09it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.74it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.24it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.81it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00,  7.69it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.85it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.86it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.50it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 10.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.72it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.73it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 10.58it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.69it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.46it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.74it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.75it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.68it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.69it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 3/3 [00:00<00:00,  8.75it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00,  7.63it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.80it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.91it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.71it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.73it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.69it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.96it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.59it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.50it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.70it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.86it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.79it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.03it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.86it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 10.89it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.93it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.45it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00,  7.46it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.06it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.03it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.61it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.94it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.77it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.90it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.67it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.49it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.79it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.85it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.71it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.06it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.73it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00,  7.61it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.86it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.60it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 3/3 [00:00<00:00,  8.64it/s]\n",
      "Evaluating: 100%|██████████| 3/3 [00:00<00:00,  8.69it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  6.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.59it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00,  9.03it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.83it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.79it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.84it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.04it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.74it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.43it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.69it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.04it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.01it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.85it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.01it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.55it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.76it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.75it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.11it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.44it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 11.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.36it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.01it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# collect all answers from reviews\n",
    "import time\n",
    "answer_collect = {}\n",
    "for key in range(1,905):\n",
    "    context = data[key]\n",
    "    test_grandson = {}\n",
    "    test_grandson['qas'] = [{'question':'What to eat?',\n",
    "                             'id': '1',\n",
    "                              'answers': [{'text':'food','answer_start': 0}],\n",
    "                              'is_impossible': False},\n",
    "                            {'question':'What to try?',\n",
    "                             'id': '2',\n",
    "                              'answers': [{'text':'food','answer_start': 0}],\n",
    "                              'is_impossible': False},\n",
    "                           {'question':'What is good?',\n",
    "                             'id': '3',\n",
    "                              'answers': [{'text':'food','answer_start': 0}],\n",
    "                              'is_impossible': False},\n",
    "                           {'question':'What is delicious?',\n",
    "                             'id': '4',\n",
    "                              'answers': [{'text':'food','answer_start': 0}],\n",
    "                              'is_impossible': False},\n",
    "                           {'question':'What to recommend?',\n",
    "                             'id': '5',\n",
    "                              'answers': [{'text':'food','answer_start': 0}],\n",
    "                              'is_impossible': False},\n",
    "                           {'question':'What do you prefer?',\n",
    "                             'id': '6',\n",
    "                              'answers': [{'text':'food','answer_start': 0}],\n",
    "                              'is_impossible': False},\n",
    "                           {'question':'How is the service?',\n",
    "                             'id': '7',\n",
    "                              'answers': [{'text':'food','answer_start': 0}],\n",
    "                              'is_impossible': False},\n",
    "                            {'question':'How is the price?',\n",
    "                             'id': '8',\n",
    "                              'answers': [{'text':'food','answer_start': 0}],\n",
    "                              'is_impossible': False},\n",
    "                           {'question':'How long to wait in this place?',\n",
    "                             'id': '9',\n",
    "                              'answers': [{'text':'food','answer_start': 0}],\n",
    "                              'is_impossible': False},\n",
    "                           {'question':'Is the place clean?',\n",
    "                             'id': '10',\n",
    "                              'answers': [{'text':'food','answer_start': 0}],\n",
    "                              'is_impossible': True}]\n",
    "    test_grandson['context'] = context\n",
    "\n",
    "    test_child = {}\n",
    "    test_child['title'] =  'Thai'\n",
    "    test_child['paragraphs'] = [test_grandson]\n",
    "\n",
    "    test = {}\n",
    "    test['version'] = 'test'\n",
    "    test['data'] = [test_child]\n",
    "    with open('test.json', 'w') as outfile:\n",
    "        json.dump(test, outfile)\n",
    "    #time.sleep(1)\n",
    "    # evaluate the QA for the new dataset\n",
    "    # evaluate the final results\n",
    "    args.predict_file = 'test.json'\n",
    "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(dataset) if args.local_rank == -1 else DistributedSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "    \n",
    "    prefix=1\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    all_results = []\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                        'attention_mask': batch[1],\n",
    "                        'token_type_ids': None if args.model_type == 'xlm' else batch[2]  # XLM don't use segment_ids\n",
    "                        }\n",
    "            example_indices = batch[3]\n",
    "            if args.model_type in ['xlnet', 'xlm']:\n",
    "                inputs.update({'cls_index': batch[4],\n",
    "                                'p_mask':    batch[5]})\n",
    "            outputs = model(**inputs)\n",
    "    \n",
    "        for i, example_index in enumerate(example_indices):\n",
    "            eval_feature = features[example_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "            if args.model_type in ['xlnet', 'xlm']:\n",
    "                # XLNet uses a more complex post-processing procedure\n",
    "                result = RawResultExtended(unique_id            = unique_id,\n",
    "                                            start_top_log_probs  = to_list(outputs[0][i]),\n",
    "                                            start_top_index      = to_list(outputs[1][i]),\n",
    "                                            end_top_log_probs    = to_list(outputs[2][i]),\n",
    "                                            end_top_index        = to_list(outputs[3][i]),\n",
    "                                            cls_logits           = to_list(outputs[4][i]))\n",
    "            else:\n",
    "                result = RawResult(unique_id    = unique_id,\n",
    "                                    start_logits = to_list(outputs[0][i]),\n",
    "                                    end_logits   = to_list(outputs[1][i]))\n",
    "            all_results.append(result)\n",
    "            \n",
    "    # Compute predictions\n",
    "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n",
    "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "    # assume some question does not have an answer\n",
    "    if args.version_2_with_negative:\n",
    "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(prefix))\n",
    "        \n",
    "    args.null_score_diff_threshold = 0\n",
    "    args.version_2_with_negative = True\n",
    "    if args.model_type in ['xlnet', 'xlm']:\n",
    "        # XLNet uses a more complex post-processing procedure\n",
    "        write_predictions_extended(examples, features, all_results, args.n_best_size,\n",
    "                        args.max_answer_length, output_prediction_file,\n",
    "                        output_nbest_file, output_null_log_odds_file, args.predict_file,\n",
    "                        model.config.start_n_top, model.config.end_n_top,\n",
    "                        args.version_2_with_negative, tokenizer, args.verbose_logging)\n",
    "    else:\n",
    "        write_predictions(examples, features, all_results, args.n_best_size,\n",
    "                        args.max_answer_length, args.do_lower_case, output_prediction_file,\n",
    "                        output_nbest_file, output_null_log_odds_file, args.verbose_logging,\n",
    "                        args.version_2_with_negative, args.null_score_diff_threshold)\n",
    "        \n",
    "    filename1 = output_prediction_file\n",
    "    with open(filename1, 'r') as f:\n",
    "        answer = json.load(f)    \n",
    "    #time.sleep(1)    \n",
    "    answer_collect[key] = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': '',\n",
       " '2': 'Thai iced tea, coconut juice, or traditional Thai beer',\n",
       " '3': '',\n",
       " '4': 'noodles',\n",
       " '5': 'beef noodle or the duck noodle',\n",
       " '6': 'beef noodle or the duck noodle',\n",
       " '7': '',\n",
       " '8': '',\n",
       " '9': '',\n",
       " '10': ''}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_collect[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save answer collection to json\n",
    "with open('answer_collect.json', 'w') as outfile:\n",
    "    json.dump(answer_collect, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('answer_collect.json', 'r') as f:\n",
    "    answer_collection = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The appetizers'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_collection['1']['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "To_eat = []\n",
    "for i in range(len(answer_collection)):\n",
    "    To_eat.append(answer_collection[str(i+1)]['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delicious = []\n",
    "for i in range(len(answer_collection)):\n",
    "    Delicious.append(answer_collection[str(i+1)]['4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recommend = []\n",
    "for i in range(len(answer_collection)):\n",
    "    Recommend.append(answer_collection[str(i+1)]['5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prefer = []\n",
    "for i in range(len(answer_collection)):\n",
    "    Prefer.append(answer_collection[str(i+1)]['6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Menu = To_eat + Delicious + recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Menu = []\n",
    "for i in range(len(To_eat)):\n",
    "    a = [To_eat[i]] + [Delicious[i]] + [Recommend[i]] + [Prefer[i]]\n",
    "    Menu += list(set(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Menu = [x.lower() for x in Menu if len(x) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Menu_filter = []\n",
    "for x in Menu:\n",
    "    if 'food' not in x.split(' '):\n",
    "        Menu_filter.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save answer collection to json\n",
    "with open('Menu_filter.json', 'w') as outfile:\n",
    "    json.dump(Menu_filter, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
