{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import os\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tools import *\n",
    "import convert_examples_to_features\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Roadwork on US 322 westbound between Oak Hall/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#NAME?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Done-On SR 99 northbound &amp;amp; southbound at 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Anybody else in Los Angeles have a fairly long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>UMMm idk where it is but its by saltyfishgirl ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  1  Roadwork on US 322 westbound between Oak Hall/...\n",
       "1  0                                             #NAME?\n",
       "2  1  Done-On SR 99 northbound &amp; southbound at 1...\n",
       "3  0  Anybody else in Los Angeles have a fairly long...\n",
       "4  0  UMMm idk where it is but its by saltyfishgirl ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('train.csv',header=None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I dont know, Arteta has said that Saka is a wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Really?? looks like Keane is the one in denial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NB &amp;amp; SB M-13 (Sheridan Rd) between Gary Rd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>same dude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Cleared | OUTAGAMIE Co | Crash | I-41 NB | MEA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  0  I dont know, Arteta has said that Saka is a wi...\n",
       "1  0  Really?? looks like Keane is the one in denial...\n",
       "2  1  NB &amp; SB M-13 (Sheridan Rd) between Gary Rd...\n",
       "3  0                                          same dude\n",
       "4  1  Cleared | OUTAGAMIE Co | Crash | I-41 NB | MEA..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv', header=None)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change label to (0,1)\n",
    "#train_df[0] = (train_df[0] == 2).astype(int)\n",
    "#test_df[0] = (test_df[0] == 2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Roadwork on US 322 westbound between Oak Hall/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>#NAME?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Done-On SR 99 northbound &amp;amp; southbound at 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Anybody else in Los Angeles have a fairly long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>UMMm idk where it is but its by saltyfishgirl ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label alpha                                               text\n",
       "0   0      1     a  Roadwork on US 322 westbound between Oak Hall/...\n",
       "1   1      0     a                                             #NAME?\n",
       "2   2      1     a  Done-On SR 99 northbound &amp; southbound at 1...\n",
       "3   3      0     a  Anybody else in Los Angeles have a fairly long...\n",
       "4   4      0     a  UMMm idk where it is but its by saltyfishgirl ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change data to tsv form (for BERT Input)\n",
    "train_df_bert = pd.DataFrame({\n",
    "    'id':range(len(train_df)),\n",
    "    'label':train_df[0],\n",
    "    'alpha':['a']*train_df.shape[0],\n",
    "    'text': train_df[1].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "\n",
    "train_df_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>I dont know, Arteta has said that Saka is a wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Really?? looks like Keane is the one in denial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>NB &amp;amp; SB M-13 (Sheridan Rd) between Gary Rd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>same dude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>Cleared | OUTAGAMIE Co | Crash | I-41 NB | MEA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label alpha                                               text\n",
       "0   0      0     a  I dont know, Arteta has said that Saka is a wi...\n",
       "1   1      0     a  Really?? looks like Keane is the one in denial...\n",
       "2   2      1     a  NB &amp; SB M-13 (Sheridan Rd) between Gary Rd...\n",
       "3   3      0     a                                          same dude\n",
       "4   4      1     a  Cleared | OUTAGAMIE Co | Crash | I-41 NB | MEA..."
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df_bert = pd.DataFrame({\n",
    "    'id':range(len(test_df)),\n",
    "    'label':test_df[0],\n",
    "    'alpha':['a']*test_df.shape[0],\n",
    "    'text': test_df[1].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "\n",
    "dev_df_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train and dev data as .tsv files.\n",
    "train_df_bert.to_csv('train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df_bert.to_csv('dev.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2147483647"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "csv.field_size_limit(2147483647) # Increase CSV reader's field limit incase we have long text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "    def get_labels(self):\n",
    "\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "\n",
    "            lines = []\n",
    "\n",
    "            for line in reader:\n",
    "\n",
    "                if sys.version_info[0] == 2:\n",
    "\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "\n",
    "                lines.append(line)\n",
    "\n",
    "            return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationProcessor(DataProcessor):\n",
    "\n",
    "    \"\"\"Processor for binary classification dataset.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "\n",
    "        \"\"\"See base class.\"\"\"\n",
    "\n",
    "        return self._create_examples(\n",
    "\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "\n",
    "        \"\"\"See base class.\"\"\"\n",
    "\n",
    "        return self._create_examples(\n",
    "\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_labels(self):\n",
    "\n",
    "        \"\"\"See base class.\"\"\"\n",
    "\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "\n",
    "        examples = []\n",
    "\n",
    "        for (i, line) in enumerate(lines):\n",
    "\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "\n",
    "            text_a = line[3]\n",
    "\n",
    "            label = line[1]\n",
    "\n",
    "            examples.append(\n",
    "\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "\n",
    "        self.input_mask = input_mask\n",
    "\n",
    "        self.segment_ids = segment_ids\n",
    "\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "\n",
    "    while True:\n",
    "\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "\n",
    "        if total_length <= max_length:\n",
    "\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "\n",
    "            tokens_a.pop()\n",
    "\n",
    "        else:\n",
    "\n",
    "            tokens_b.pop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(example_row):\n",
    "\n",
    "    # return example_row\n",
    "\n",
    "    example, label_map, max_seq_length, tokenizer, output_mode = example_row\n",
    "\n",
    "\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "\n",
    "\n",
    "    tokens_b = None\n",
    "\n",
    "    if example.text_b:\n",
    "\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "\n",
    "        # length is less than the specified length.\n",
    "\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "\n",
    "\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "\n",
    "    segment_ids = [0] * len(tokens)\n",
    "\n",
    "\n",
    "\n",
    "    if tokens_b:\n",
    "\n",
    "        tokens += tokens_b + [\"[SEP]\"]\n",
    "\n",
    "        segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "\n",
    "    # tokens are attended to.\n",
    "\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "    input_ids += padding\n",
    "\n",
    "    input_mask += padding\n",
    "\n",
    "    segment_ids += padding\n",
    "\n",
    "\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "\n",
    "    assert len(input_mask) == max_seq_length\n",
    "\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "\n",
    "\n",
    "    if output_mode == \"classification\":\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "\n",
    "    elif output_mode == \"regression\":\n",
    "\n",
    "        label_id = float(example.label)\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise KeyError(output_mode)\n",
    "\n",
    "\n",
    "\n",
    "    return InputFeatures(input_ids=input_ids,\n",
    "\n",
    "                         input_mask=input_mask,\n",
    "\n",
    "                         segment_ids=segment_ids,\n",
    "\n",
    "                         label_id=label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data dir. Should contain the .tsv files (or other data files) for the task.\n",
    "DATA_DIR = \"/home/xwan6/Bert_classify/\"\n",
    "\n",
    "# Bert pre-trained model selected in the list: bert-base-uncased, \n",
    "# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n",
    "# bert-base-multilingual-cased, bert-base-chinese.\n",
    "BERT_MODEL = 'bert-base-cased'\n",
    "\n",
    "# The name of the task to train.I'm going to name this 'yelp'.\n",
    "TASK_NAME = 'yelp'\n",
    "\n",
    "# The output directory where the fine-tuned model and checkpoints will be written.\n",
    "OUTPUT_DIR = f'outputs/{TASK_NAME}/'\n",
    "\n",
    "# The directory where the evaluation reports will be written to.\n",
    "REPORTS_DIR = f'reports/{TASK_NAME}_evaluation_report/'\n",
    "\n",
    "# This is where BERT will look for pre-trained models to load parameters from.\n",
    "CACHE_DIR = 'cache/'\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization.\n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "TRAIN_BATCH_SIZE = 24\n",
    "EVAL_BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "RANDOM_SEED = 42\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "WARMUP_PROPORTION = 0.1\n",
    "OUTPUT_MODE = 'classification'\n",
    "\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mode = OUTPUT_MODE\n",
    "cache_dir = CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(REPORTS_DIR) and os.listdir(REPORTS_DIR):\n",
    "        REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
    "        os.makedirs(REPORTS_DIR)\n",
    "if not os.path.exists(REPORTS_DIR):\n",
    "    os.makedirs(REPORTS_DIR)\n",
    "    REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
    "    os.makedirs(REPORTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(OUTPUT_DIR) and os.listdir(OUTPUT_DIR):\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(OUTPUT_DIR))\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BinaryClassificationProcessor()\n",
    "train_examples = processor.get_train_examples(DATA_DIR)\n",
    "train_examples_len = len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = processor.get_labels() # [0, 1] for binary classification\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_optimization_steps = int(\n",
    "    train_examples_len / TRAIN_BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/xwan6/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "train_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in train_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to convert 96518 examples..\n",
      "Spawning 31 processes..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ee2a4d02c44323a99298443901ee10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=96518), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# process the examples with multiple cpu (slightly fast)\n",
    "process_count = cpu_count() - 1\n",
    "if __name__ ==  '__main__':\n",
    "    print(f'Preparing to convert {train_examples_len} examples..')\n",
    "    print(f'Spawning {process_count} processes..')\n",
    "    with Pool(process_count) as p:\n",
    "        train_features = list(tqdm_notebook(p.imap(convert_examples_to_features.convert_example_to_feature, train_examples_for_processing), total=train_examples_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pickle\n",
    "#with open(DATA_DIR + \"train_features.pkl\", \"wb\") as f:\n",
    "    #pickle.dump(train_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickle\n",
    "with open(DATA_DIR + \"train_features.pkl\", \"rb\") as f:\n",
    "    train_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at cache/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file cache/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir /tmp/tmprum3pxkd\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, cache_dir=CACHE_DIR, num_labels=num_labels)\n",
    "# model = BertForSequenceClassification.from_pretrained(CACHE_DIR + 'cased_base_bert_pytorch.tar.gz', cache_dir=CACHE_DIR, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the default parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=LEARNING_RATE,\n",
    "                     warmup=WARMUP_PROPORTION,\n",
    "                     t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:***** Running training *****\n",
      "INFO:root:  Num examples = 96518\n",
      "INFO:root:  Batch size = 24\n",
      "INFO:root:  Num steps = 4021\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", train_examples_len)\n",
    "logger.info(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
    "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "\n",
    "if OUTPUT_MODE == \"classification\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our DataLoader for training..\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b7f90425f14a94afadf91003e1b90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=4022, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [16:30<00:00, 990.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training time\n",
    "model.train()\n",
    "for _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "    loss_record = []\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm_notebook(train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "        if OUTPUT_MODE == \"classification\":\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "        elif OUTPUT_MODE == \"regression\":\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        loss.backward()\n",
    "        print(\"\\r%f\" % loss, end='')\n",
    "        # get the loss for each steps\n",
    "        loss_record.append(loss.item())\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_record[:1000])\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/yelp/vocab.txt'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "model_to_save.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "#output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
    "#output_config_file = os.path.join(OUTPUT_DIR, CONFIG_NAME)\n",
    "\n",
    "#torch.save(model_to_save.state_dict(), output_model_file)\n",
    "#model_to_save.config.to_json_file(output_config_file)\n",
    "#tokenizer.save_vocabulary(OUTPUT_DIR)\n",
    "\n",
    "# load the model\n",
    "#model_type = 'bert'\n",
    "#MODEL_CLASSES = {\n",
    "#    'bert': (BertConfig, BertForQuestionAnswering, BertTokenizer),\n",
    "#    'xlnet': (XLNetConfig, XLNetForQuestionAnswering, XLNetTokenizer),\n",
    "#    'xlm': (XLMConfig, XLMForQuestionAnswering, XLMTokenizer),\n",
    "#    'distilbert': (DistilBertConfig, DistilBertForQuestionAnswering, DistilBertTokenizer)\n",
    "#}\n",
    "#config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "#config = config_class.from_pretrained(BERT_MODEL)\n",
    "#model = model_class.from_pretrained(Output_dir)\n",
    "#tokenizer = tokenizer_class.from_pretrained(Output_dir, do_lower_case=True)\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from tools import *\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import convert_examples_to_features\n",
    "\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import os\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data dir. Should contain the .tsv files (or other data files) for the task.\n",
    "DATA_DIR = \"/home/xwan6/Bert_classify/\"\n",
    "\n",
    "# Bert pre-trained model selected in the list: bert-base-uncased, \n",
    "# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n",
    "# bert-base-multilingual-cased, bert-base-chinese.\n",
    "#BERT_MODEL = 'bert-base-cased'\n",
    "\n",
    "# The name of the task to train.I'm going to name this 'yelp'.\n",
    "TASK_NAME = 'yelp'\n",
    "\n",
    "# The output directory where the fine-tuned model and checkpoints will be written.\n",
    "OUTPUT_DIR = '/home/xwan6/Bert_classify/outputs/yelp/'\n",
    "\n",
    "# The directory where the evaluation reports will be written to.\n",
    "REPORTS_DIR = '/home/xwan6/Bert_classify/reports/yelp_evaluation_reports/'\n",
    "\n",
    "# This is where BERT will look for pre-trained models to load parameters from.\n",
    "CACHE_DIR = 'cache/'\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization.\n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "TRAIN_BATCH_SIZE = 24\n",
    "EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "RANDOM_SEED = 42\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "WARMUP_PROPORTION = 0.1\n",
    "OUTPUT_MODE = 'classification'\n",
    "\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(REPORTS_DIR) and os.listdir(REPORTS_DIR):\n",
    "        REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
    "        os.makedirs(REPORTS_DIR)\n",
    "if not os.path.exists(REPORTS_DIR):\n",
    "    os.makedirs(REPORTS_DIR)\n",
    "    REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
    "    os.makedirs(REPORTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_report(task_name, labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    return {\n",
    "        \"task\": task_name,\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn\n",
    "    }\n",
    "\n",
    "def compute_metrics(task_name, labels, preds):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(task_name, labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we must remove wrap cell in the csv in order to process\n",
    "processor = BinaryClassificationProcessor()\n",
    "eval_examples = processor.get_dev_examples(DATA_DIR)\n",
    "label_list = processor.get_labels() # [0, 1] for binary classification\n",
    "num_labels = len(label_list)\n",
    "eval_examples_len = len(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "eval_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in eval_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to convert 10000 examples..\n",
      "Spawning 31 processes..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed9f240f5294fa48bc5536b8510c5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_count = cpu_count() - 1\n",
    "if __name__ ==  '__main__':\n",
    "    print(f'Preparing to convert {eval_examples_len} examples..')\n",
    "    print(f'Spawning {process_count} processes..')\n",
    "    with Pool(process_count) as p:\n",
    "        eval_features = list(tqdm_notebook(p.imap(convert_examples_to_features.convert_example_to_feature, eval_examples_for_processing), total=eval_examples_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_MODE == \"classification\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "#model = BertForSequenceClassification.from_pretrained(CACHE_DIR + BERT_MODEL, cache_dir=CACHE_DIR, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80e7c9b78d94fb6ba8f9c8ef2f1490b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1250, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:***** Eval results *****\n",
      "INFO:root:  task = yelp\n",
      "INFO:root:  mcc = 0.9989981785404111\n",
      "INFO:root:  tp = 4774\n",
      "INFO:root:  tn = 5221\n",
      "INFO:root:  fp = 1\n",
      "INFO:root:  fn = 4\n",
      "INFO:root:  eval_loss = 0.0025233404874801637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    # create eval loss and other metric required by the task\n",
    "    if OUTPUT_MODE == \"classification\":\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "    elif OUTPUT_MODE == \"regression\":\n",
    "        loss_fct = MSELoss()\n",
    "        tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    nb_eval_steps += 1\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "preds = preds[0]\n",
    "if OUTPUT_MODE == \"classification\":\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    preds = np.squeeze(preds)\n",
    "result = compute_metrics(TASK_NAME, all_label_ids.numpy(), preds)\n",
    "\n",
    "result['eval_loss'] = eval_loss\n",
    "\n",
    "output_eval_file = os.path.join(REPORTS_DIR, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in (result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Inside HT's policy briefing: Lobbying Act revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Oh boy oh boy oh boy I wonder who the next spr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Games just issuing delay statements daily now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Did they actually just cite a problem near the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>We're sorry there was a delay in our response,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label alpha                                               text\n",
       "0   0      0     a  Inside HT's policy briefing: Lobbying Act revi...\n",
       "1   1      0     a  Oh boy oh boy oh boy I wonder who the next spr...\n",
       "2   2      0     a      Games just issuing delay statements daily now\n",
       "3   3      0     a  Did they actually just cite a problem near the...\n",
       "4   4      0     a  We're sorry there was a delay in our response,..."
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we use our model to extract related information\n",
    "import pandas as pd\n",
    "test_df = pd.read_csv('example.csv')\n",
    "# change data to tsv form (for BERT Input)\n",
    "test_df_bert = pd.DataFrame({\n",
    "    'id':range(len(test_df)),\n",
    "    'label':[0]*test_df.shape[0],\n",
    "    'alpha':['a']*test_df.shape[0],\n",
    "    'text': test_df['text'].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "test_df_bert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the test data as .tsv files.\n",
    "test_df_bert.to_csv('dev.tsv', sep='\\t', index=False, header=False)\n",
    "processor = BinaryClassificationProcessor()\n",
    "eval_examples = processor.get_dev_examples(DATA_DIR)\n",
    "label_list = processor.get_labels() # [0, 1] for binary classification\n",
    "num_labels = len(label_list)\n",
    "eval_examples_len = len(eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to convert 10000 examples..\n",
      "Spawning 31 processes..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916c4118f4fa4c80b3927afaa1d26485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "eval_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in eval_examples]\n",
    "\n",
    "process_count = cpu_count() - 1\n",
    "if __name__ ==  '__main__':\n",
    "    print(f'Preparing to convert {eval_examples_len} examples..')\n",
    "    print(f'Spawning {process_count} processes..')\n",
    "    with Pool(process_count) as p:\n",
    "        eval_features = list(tqdm_notebook(p.imap(convert_examples_to_features.convert_example_to_feature, eval_examples_for_processing), total=eval_examples_len))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "\n",
    "if OUTPUT_MODE == \"classification\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0373b2ddcfce4120a465b7c9836fef3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=1250, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "if OUTPUT_MODE == \"classification\":\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    preds = np.squeeze(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(preds)\n",
    "result = result.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for one sentence\n",
    "#text = 'I will visit the transportation office tomorrow at Garden parkay but my car is crashed.'\n",
    "text = 'An accident on 73 SB approaching Fellowship Rd is causing a delay between 295 and the NJ Turnpike.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ex = []\n",
    "for i in range(10):\n",
    "    eval_ex.append(InputExample('1',text,None,'0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ex_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in eval_ex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to convert 1 examples..\n",
      "Spawning 31 processes..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175ebdb1271f47fb974c3fd8d4688042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "process_count = cpu_count() - 1\n",
    "if __name__ ==  '__main__':\n",
    "    print(f'Preparing to convert {1} examples..')\n",
    "    print(f'Spawning {process_count} processes..')\n",
    "    with Pool(process_count) as p:\n",
    "        eval_features = list(tqdm_notebook(p.imap(convert_examples_to_features.convert_example_to_feature, eval_ex_for_processing), total=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08a0e6da382481691ce62f6cfc0d8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=2, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = preds[0]\n",
    "preds = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
